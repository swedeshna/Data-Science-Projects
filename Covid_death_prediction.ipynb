{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd \n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import datetime\n",
    "import operator\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\\\Home\\\\Desktop\\\\DataTrained\\\\covid_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1/22/2020</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1/22/2020</td>\n",
       "      <td>Albania</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1/22/2020</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1/22/2020</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1/22/2020</td>\n",
       "      <td>Angola</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date      Country  Confirmed  Recovered  Deaths\n",
       "0  1/22/2020  Afghanistan          0          0       0\n",
       "1  1/22/2020      Albania          0          0       0\n",
       "2  1/22/2020      Algeria          0          0       0\n",
       "3  1/22/2020      Andorra          0          0       0\n",
       "4  1/22/2020       Angola          0          0       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_increase_rate(csv_file: str = \"C:\\\\Users\\\\\\Home\\\\Desktop\\\\DataTrained\\\\covid_dataset.csv\"):\n",
    "    with open(csv_file) as read_file:\n",
    "        rows = read_file.read().splitlines()\n",
    "    with open(csv_file, \"w\") as write_file:\n",
    "        header_row = rows[0] + \",Increase rate\"\n",
    "        write_file.write(f\"{header_row}\\n\")\n",
    "        previous_row = None\n",
    "        for row in rows[1:]:  # exclude header row\n",
    "            if previous_row:\n",
    "                prev_confirmed = int(previous_row.split(\",\")[1])\n",
    "                confirmed = int(row.split(\",\")[1])\n",
    "                rate = (confirmed - prev_confirmed) / prev_confirmed * 100\n",
    "                write_file.write(f\"{row},{rate}\\n\")\n",
    "            else:\n",
    "                write_file.write(f\"{row},\\n\")\n",
    "            previous_row = row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_date(s):\n",
    "    l = s.split(\"/\")\n",
    "    return f\"20{l[2]}-{int(l[0]):02d}-{int(l[1]):02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/\"\n",
    "confirmed_url = \"time_series_covid19_confirmed_global.csv\"\n",
    "dead_url = \"time_series_covid19_deaths_global.csv\"\n",
    "recovered_url = \"time_series_covid19_recovered_global.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "Working on basic time series\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"===============\\nWorking on basic time series\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_url + confirmed_url)\n",
    "confirmed_copy = df.copy()  # for time series file with provinces\n",
    "df = df.drop([\"Lat\", \"Long\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_concat = df[df[\"Province/State\"].notna()][\"Country/Region\"].unique()\n",
    "# We have to combine the numbers from the countries that are split up into\n",
    "# provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in to_concat:\n",
    "    new_row = df[df[\"Country/Region\"] == country].sum()\n",
    "    new_row[\"Country/Region\"] = country\n",
    "    new_row[\"Province/State\"] = np.NaN\n",
    "    df = df.drop(df[df[\"Country/Region\"] == country].index)\n",
    "    df = df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed = df[df[\"Province/State\"].isna()].drop(\n",
    "    \"Province/State\", axis=1\n",
    ")  # take only countries (no territories)\n",
    "# print(to_concate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_url + dead_url)\n",
    "dead_copy = df.copy()  # for time series file with provinces\n",
    "df = df.drop([\"Lat\", \"Long\"], axis=1)\n",
    "to_concat = df[df[\"Province/State\"].notna()][\"Country/Region\"].unique()\n",
    "for country in to_concat:\n",
    "    new_row = df[df[\"Country/Region\"] == country].sum()\n",
    "    new_row[\"Country/Region\"] = country\n",
    "    new_row[\"Province/State\"] = np.NaN\n",
    "    df = df.drop(df[df[\"Country/Region\"] == country].index)\n",
    "    df = df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = df[df[\"Province/State\"].isna()].drop(\n",
    "    \"Province/State\", axis=1\n",
    ")  # take only countries (no territories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all values in the column are strings (np.NaN is a float)\n",
    "confirmed[\"Country/Region\"] = confirmed[\"Country/Region\"].fillna(\"\")\n",
    "dead[\"Country/Region\"] = dead[\"Country/Region\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_url + recovered_url)\n",
    "recovered_copy = df.copy()  # for time series\n",
    "df = df.drop([\"Lat\", \"Long\"], axis=1)\n",
    "to_concat = df[df[\"Province/State\"].notna()][\"Country/Region\"].unique()\n",
    "for country in to_concat:\n",
    "    new_row = df[df[\"Country/Region\"] == country].sum()\n",
    "    new_row[\"Country/Region\"] = country\n",
    "    new_row[\"Province/State\"] = np.NaN\n",
    "    df = df.drop(df[df[\"Country/Region\"] == country].index)\n",
    "    df = df.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered = df[df[\"Province/State\"].isna()].drop(\n",
    "    \"Province/State\", axis=1\n",
    ")  # take only countries (no territories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
